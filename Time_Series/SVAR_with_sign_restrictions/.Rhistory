labs(x="", # No x-title
caption = "Source: Federal Reserve Bank of St.Louis and author's calculations. Shaded areas indicate US recessions. Quarterly data, 1995Q1-2019Q1.") +
theme_bw() +
theme(plot.title = element_text(color="black", size=14, face="italic"),
axis.text.x = element_text(angle=45, vjust = 0.5),
axis.title.y = element_text(color = mycolors["USEPUINDXD"]),
axis.text.y = element_text(color = mycolors["USEPUINDXD"]),
axis.title.y.right = element_text(color = mycolors["STLFSI"]),
axis.text.y.right = element_text(color = mycolors["STLFSI"]),
panel.grid.major.y = element_line(linetype=3),
panel.grid.minor.y = element_line(linetype=3),
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
legend.background = element_rect(fill = "transparent"), # get rid of legend bg
legend.key = element_rect(fill = "transparent", colour = NA), # get rid of key legend fill, and of the surrounding
legend.title=element_blank(),
legend.position = c(0.19,0.90),
plot.caption = element_text(hjust = 0) # Left-align caption/notes
)
figure_1 <- reposition_legend(figure_1, 'top left')
View(figure_1)
View(figure_1)
figure_1
bdirect <- c(10 - A, A, 10 - B)
a;b;c;d;e;f <- NA
a<- NA;b<- NA;c<- NA;d<- NA;e<- NA;f <- NA
bdirect <- c(10 - a, a, 10 - b)
bdirect
10 - a
bdirect <- c((10 - a), a, 10 - b)
library(tidyverse)
library(zoo) # zoo contains the function as.yearqtr
library(purrr) # for map_dfr
library(lemon) # Better legend in ggplot2
# Use this package
# install.packages("fredr")
library(fredr)
######################
###########   IMPORTANT !!!!!!!!!!!!!!!!!!!!!
# Always run at the beginning of each session: this is the API key provided by the FRED
fredr_set_key("668409be20b0f03136a4f47a921c680c")
###########
risk <- fredr_series_observations(   # financial stress
series_id = "NFCIRISK",
observation_start = as.Date("1995-01-01"),
frequency = "q",
units = "lin"
)
# Make quarterly data as YYYYQ
risk  <- transform(risk, date = as.yearqtr(date))
# Credit subindex
credit <- fredr_series_observations(   # financial stress
series_id = "NFCICREDIT",
observation_start = as.Date("1995-01-01"),
frequency = "q",
units = "lin"
)
# Make quarterly data as YYYYQ
credit  <- transform(credit, date = as.yearqtr(date))
# Leverage subindex
leverage <- fredr_series_observations(
series_id = "NFCILEVERAGE",
observation_start = as.Date("1995-01-01"),
frequency = "q",
units = "lin"
)
# Make quarterly data as YYYYQ
leverage  <- transform(leverage, date = as.yearqtr(date))
# Charge-off rate
choff <- fredr_series_observations(   # financial stress
series_id = "CORSREACBS",
observation_start = as.Date("1995-01-01"),
frequency = "q",
units = "lin"
)
# Make quarterly data as YYYYQ
choff  <- transform(choff, date = as.yearqtr(date))
# Merge
datafed <- rbind(risk,credit,leverage,choff)
mycolors <- c("NFCILEVERAGE"="goldenrod3","NFCICREDIT"="deeppink2","NFCIRISK"="forestgreen","CORSREACBS"="blue")
View(datafed)
View(datafed)
View(datafed)
roberto <- function(x,y){
mat <- x+y
return(mat)
}
ciao <- roberto(20,20)
ciao
roberto(20,20,100)
roberto <- function(x,y,z){
mat <- x+y+z
return(mat)
}
roberto(20,20,100)
######################
# Library
library(tidyverse)
library(zoo) # zoo contains the function as.yearqtr
library(purrr) # for map_dfr
library(lemon) # Better legend in ggplot2
# Use this package
# install.packages("fredr")
library(fredr)
library(lubridate)
######################
rm(list=ls())
# Risk subindex
gdp <- fredr_series_observations(   # financial stress
series_id = "GDP",
observation_start = as.Date("1995-01-01"),
frequency = "q",
units = "lin"
)
# Make quarterly data as YYYYQ
gdp  <- transform(gdp, date = as.yearqtr(date))
# Credit subindex
potgdp <- fredr_series_observations(   # financial stress
series_id = "NGDPPOT",
observation_start = as.Date("1995-01-01"),
frequency = "q",
units = "lin"
)
# Make quarterly data as YYYYQ
potgdp  <- transform(potgdp, date = as.yearqtr(date))
fredr_set_key("668409be20b0f03136a4f47a921c680c")
###########
rm(list=ls())
# Risk subindex
gdp <- fredr_series_observations(   # financial stress
series_id = "GDP",
observation_start = as.Date("1995-01-01"),
frequency = "q",
units = "lin"
)
# Make quarterly data as YYYYQ
gdp  <- transform(gdp, date = as.yearqtr(date))
# Credit subindex
potgdp <- fredr_series_observations(   # financial stress
series_id = "NGDPPOT",
observation_start = as.Date("1995-01-01"),
frequency = "q",
units = "lin"
)
# Make quarterly data as YYYYQ
potgdp  <- transform(potgdp, date = as.yearqtr(date))
View(gdp)
View(potgdp)
potgdp <- fredr_series_observations(   # financial stress
series_id = "NGDPPOT",
observation_start = as.Date("1995-01-01"),
observation_end = as.Date("2019-01-01"),
frequency = "q",
units = "lin"
)
# Credit subindex
potgdp <- fredr_series_observations(   # financial stress
series_id = "NGDPPOT",
observation_start = as.Date("1995-01-01"),
observation_end = as.Date("2018-12-01"),
frequency = "q",
units = "lin"
)
View(potgdp)
potgdp  <- transform(potgdp, date = as.yearqtr(date))
View(potgdp)
gap <- potgdp-gdp
gap <- potgdp[,3]-gdp[,3]
perc_gap <- (gdp[,3]-potgdp[,3])/potgdp[,3]
gap <- rbind(gdp[,1:2],potgdp)
gap <- rbind(gdp[,1:2],gap)
View(gap)
gap <- gdp[,3]-potgdp[,3]
gap <- gdp[,3]-potgdp[,3] %>% as.data.frame()
gap$series_id <- rep("GDP_GAP", nrow(gap))
View(gap)
gap <- rbind(gdp[,1],gap)
View(gap)
gap <- rbind(gdp[,1],gap)
View(gap)
# I build a function to create nice date formats
make_monthly <- function(start,end,rows,columns,dataset,vardate){
months.list <- c("01","02","03","04","05","06","07","08","09","10","11","12")
years.list <- c(start:end)
#The next line of code generates all the month-year combinations.
df<-expand.grid(year=years.list,month=months.list)
#Then, we paste together the year and month with a day so that we get dates like "2007-03-01".  Pass that to as.Date, and pass the result to as.yearqtr.
df$Date=as.yearmon(as.Date(paste0(df$year,"-",df$month,"-01")))
df <- df[order(df$Date),]
df <- df[rows,columns]
date <- df
dataset$date <- date
# Correct Formatting
dataset$date <- as.yearmon(dataset$date)
dataset$series_id <- as.character(dataset$series_id)
dataset$value <- as.numeric(as.character(dataset$value))
return(dataset)
}
View(gap)
gap <- gdp[,3]-potgdp[,3] %>% as.data.frame()
gap$series_id <- rep("GDP_GAP", nrow(gap))
gap <- rbind(gdp[,1],gap)
colnames(gap) <- c("value","series_id")
gap <- rbind(gdp[,1],gap)
View(gap)
gap <- gdp[,3]-potgdp[,3] %>% as.data.frame()
gap$series_id <- rep("GDP_GAP", nrow(gap))
colnames(gap) <- c("value","series_id")
View(gap)
gap$date <- gdp[,1]
View(gap)
gap <- cbind([,3],[,2],[,1])
library(tidyverse) # many packages bundled into one
camilo <- function(x,y){
z <- rbind(x,y)
return(z)
}
knitr::opts_chunk$set(echo = FALSE,
message = FALSE,
warning = FALSE,
cache = T,
fig.align="center")
library(FE)
library(TTR,quantmod)
library(dlm)
library(tidyverse)
library(stats)
library(forecast)
library(knitr)
library(kableExtra)
library(gridExtra)
library(psych)
# install.packages("EnvStats")
library(EnvStats) # for Q-Q plot
library(YieldCurve)
# install.packages("sarima")
library(sarima)
library(aTSA)
library(rugarch)
# install.packages("fGarch")
library(fGarch)
library(tseries)
library(urca)
library(graphics)
library(MASS)
library(strucchange)
library(sandwich)
library(lmtest)
library(vars)
library(tsDyn)
library(fUnitRoots)
data <- read.table("Dividends.dat", header=T, stringsAsFactors=F)
data <- ts(data[,2:61], start = c(1966, 1), frequency = 4)
dps <- data[,1:15]
eps <- data[,16:30]
prices <- data[,31:45]
# Equally-weighted average
make_index <- function(series){
ind <- apply(series,1,mean) %>%
ts(start = c(1966, 1), frequency = 4)
return(ind)
}
# Price index
pindex  <- make_index(prices)
# Dividends
dpsindex <- make_index(dps)
# EPS
epsindex <- make_index(eps)
# Log dividend (net) growth rate
div_growth <- (diff(log(dpsindex))) %>%
ts(start = c(1966, 2), frequency = 4)
# Log dividend-price ratio
dp_ratio <- log(dpsindex/pindex) %>%
ts(start = c(1966, 1), frequency = 4)
# Earnings-price ratio
ep_ratio <- (epsindex/pindex) %>%
ts(start = c(1966, 1), frequency = 4)
par(mar=c(2,2,2,2))  # Regulate margins
par(mfrow=c(1,1))    # Reset environment
ggtsdisplay(div_growth, main="Log Dividend Growth Rate", points = F,
theme = theme_minimal())
ggtsdisplay(dp_ratio, main="Log Dividend-Price Ratio", points = F,
theme = theme_minimal())
ggtsdisplay(ep_ratio, main="Earnings-Price Ratio", points = F,
theme = theme_minimal())
# Long-Horizon Regressions div_growth (moving (=overlapping) log div_growth)
# Popular forecasting variables: slide 24/45, Lecture 6
# Price-dividend ratio
pd_ratio <- (pindex/dpsindex)
# Price-earnings ratio
pe_ratio <- (pindex/epsindex)
# Spread between long- and short-term interst rates
# Start from quarter 1 of 1982 for simplicity in calculating quarters upon aggregation
data("FedYieldCurve")
FedYieldCurve <- FedYieldCurve[2:nrow(FedYieldCurve),]
monthly <- ts(FedYieldCurve,start=c(1982,1),frequency=12)
quarterly <- aggregate(monthly, nfrequency=4)
# R_10Y  -  R_3M
spread <- quarterly[,1] - quarterly[,8]
# Function to compute Long-Horizon Regressions
lr_pred <- function(series,horizon){
tmp <- matrix(NA,nrow( 100*(diff(log(dpsindex),lag=horizon)) %>% as.matrix() %>%
ts(start = c(1966, 2), frequency = 4)),1)
tmp <- 100*(diff(log(dpsindex),lag=horizon)) %>% as.matrix() %>%
ts(start = c(1966, 2), frequency = 4)
tmpp <- as.matrix(series) %>%
ts(start = c(1966, 2), frequency = 4, end = end(tmp))
tmppp <- cbind(horizon,t(summary(lm(tmp~tmpp))[["coefficients"]][2,]),
summary(lm(tmp~tmpp))[["r.squared"]])
#summary(lm(tmp~tmpp))[["r.squared"]])
return(tmppp)
}
# It would also be possible to compute HAC-robust statistics and coefficient estimates
# I omit the step for brevity.
# lm(tmp~tmpp)
# coeftest(fit,vcov=NeweyWest(fit,verbose=T))
# Create output
out <- function(series){
horizons <- c(8, 20, 40, 48, 60)
# 2-year, 5-year, 10-years, 12-year, 15-year horizons
tmpmat <- matrix(NA, 5, 6)
colnames(tmpmat) <- c("Horizon (Quarters)","Coeff.", "SD", "t-stat", "p-value", "R-sq.")
for (iter in 1:5){
tmpmat[iter,]<- lr_pred(series,horizons[iter])
}
return(tmpmat)
}
# Only for spreads, since the time series is shorter I adjust the function a bit
lr_pred_spread <- function(series,horizon){
tmp <- matrix(NA,nrow( 100*(diff(log(dpsindex),lag=horizon)) %>% as.matrix() %>%
ts(start = c(1982, 1), frequency = 4)),1)
tmp <- 100*(diff(log(dpsindex),lag=horizon)) %>% as.matrix() %>%
ts(start = c(1982, 1), frequency = 4)
tmpp <- as.matrix(series) %>%
ts(start = c(1982, 2), frequency = 4, end = end(tmp))
tmppp <- cbind(horizon,t(summary(lm(tmp~tmpp))[["coefficients"]][2,]),
summary(lm(tmp~tmpp))[["r.squared"]])
#summary(lm(tmp~tmpp))[["r.squared"]])
return(tmppp)
}
# Create output
out_spread <- function(series){
horizons <- c(8, 20, 40)
tmpmat <- matrix(NA, 3, 6)
colnames(tmpmat) <- c("Horizon","Coeff.", "SD", "t-stat", "p-value", "R-sq.")
for (iter in 1:3){
tmpmat[iter,]<- lr_pred(series,horizons[iter])
}
return(tmpmat)
}
lr_output <- rbind(
out(pd_ratio),
out(pe_ratio),
out(dp_ratio),
out_spread(spread)
)
kable(lr_output, digits = 4, format = "html", caption = "Long-Horizon Regressions (Dep. Var: Log Dividend Growth Rate)",
align = "lccccc") %>%
kable_styling(c("striped","condensed")) %>%
group_rows("Panel A: Price-Dividend Ratio", 1, 5, label_row_css = "background-color: #521; color: #fff;") %>%
group_rows("Panel B: Price-Earnings Ratio",6, 10, label_row_css = "background-color: #521; color: #fff;") %>%
group_rows("Panel C: Log Dividend-Price Ratio",11, 15, label_row_css = "background-color: #521; color: #fff;") %>%
group_rows("Panel D: Yield Spread",16, 18, label_row_css = "background-color: #521; color: #fff;") %>%
#row_spec(c(2:5,12:15), bold = T, color = "white", background = "#367") %>%
scroll_box(width = "100%", height = "400px")
# VAR
returns <- data[,46:ncol(data)]; retindex <- (make_index(returns))
# Log returns, log dividend growth rate, E/P ratio and log D/P ratio
vardata <- (cbind(retindex, div_growth,ep_ratio,dp_ratio))
vardata <- na.omit(vardata)
# Fit VAR(1) + constant
varFE <- VAR(vardata, p = 1, type = "const")
# The Forecast (2 quarters) error variance decomposition is more interesting than the estimated coefficients
vdecompo <- matrix(NA,8,5)
vdecompo[,1] <- hor <- rep(c(1,2),4)
colnames(vdecompo) <-c("Quarters Ahead","Log Returns","Log Div. Growth Rate","E/P Ratio", " Log D/P Ratio")
vdecompo[1:2,2:5] <- fevd(varFE, n.ahead = 2)$retindex*100
vdecompo[3:4,2:5] <- fevd(varFE, n.ahead = 2)$div_growth*100
vdecompo[5:6,2:5] <- fevd(varFE, n.ahead = 2)$ep_ratio*100
vdecompo[7:8,2:5] <- fevd(varFE, n.ahead = 2)$dp_ratio*100
kable(vdecompo, digits = 4, format = "html", caption = "Forecast Error Variance decomposition",
align = "lcccc", col.names = colnames(vdecompo))  %>%
kable_styling(c("striped","condensed")) %>%
group_rows("Panel A: Log Returns", 1, 2, label_row_css = "background-color: #333; color: #fff;") %>%
group_rows("Panel B: Log Dividend Growth Rate",3, 4, label_row_css = "background-color: #333; color: #fff;") %>%
group_rows("Panel C: Earnings-Price Ratio",5, 6, label_row_css = "background-color: #333; color: #fff;") %>%
group_rows("Panel D: Dividend-Price Ratio",7, 8, label_row_css = "background-color: #333; color: #fff;") %>%
scroll_box(width = "100%", height = "400px")
summary_VAR <- summary(varFE)[["corres"]] %>% as.matrix()
colnames(summary_VAR) <- c("Log Returns","Log Div. Growth Rate","E/P Ratio", " Log D/P Ratio")
rownames(summary_VAR) <- colnames(summary_VAR)
kable(summary_VAR, digits = 4, format = "html", caption = "Correlation Matrix VAR(1)",
align = "lcccc") %>%
kable_styling(c("striped","condensed")) %>%
scroll_box(width = "100%", height = "300px")
Research_Interests <-c("Macroeconometrics","Monetary Economics",
"International Finance","Computational Macroeconomics")
Research_Interests
Research_Interests <-as.matrix(c("Macroeconometrics","Monetary Economics",
"International Finance","Computational Macroeconomics"))
View(Research_Interests)
solve_macroproblems <- matrix(NA,5,1)
for (iter in 1:5){
solve_macroproblems[iter,] <- 6
}
solve_macroproblems
solve_macroproblems <- matrix(NA,5,1)
for (iter in 1:5){
solve_macroproblems[iter] <- 6
}
solve_macroproblems
barplot(model_uhlig$BDraws)
library(VARsignR) # by Christian Danne
#######
# Opening routine
setwd("~/codes_github/SVAR_with_sign_restrictions")
rm(list = ls())
# Import data
stockwatson <- read.delim("sw2001.txt", header=TRUE)
swdata <- ts(stockwatson[ ,c(2:4)], start = c(1960,1), frequency = 4)
##############################
# Specify sign restrictions
#############################
# In vector -constr the first element is the sign restriction imposed on the variable whose shock I am looking to identify.
# Since the variables appear in the following order: Inflation, Unemployment, and Fed Funds, +3 says that
# a contractionary monetary policy shock makes the Fed Funds rate (weakly) higher for at least some quarters after the shock.
# -1 says that I impose a (weakly) negative sign restriction on the response of inflation to the MP shock, based on the prediction from economic theory
# that the shock will push inflation down for at least some quarters after the shock.
# I impose no sign restriction on the response of the unemployment rate (variable 2 in the dataset), but I could.
constr <- c(+3,-1)
# KMIN and KMAX define the first and the last periods in the repsonses to which restrictions are applied.
# First, I estimate a BVAR model with a flat normal inverted-Wishart prior and identify structural monetary policy shock using Uhlig (2005) rejection method
model_uhlig <- uhlig.reject(Y=swdata, nlags=12, draws=200, subdraws=200, nkeep=1000, KMIN=1,
KMAX=6, constrained=constr, constant=FALSE, steps=60)
# Alternatively, one can use Rubio-Ramirez et al. (2010) rejection method.
model_rwz<- rwz.reject(Y=swdata, nlags=12, draws=200, subdraws=200, nkeep=1000,
KMIN=1, KMAX=6, constrained=constr, constant=FALSE, steps=60)
# All commands below are valid also for model2.
# The MCMC sampler stops either teh desiered number of draws that satisfy all sign restrictions at the same time have drawn
# or when the maximum number of draws has been reached.
# BDraws: posterior draws of the coefficients of the model.
# SDraws: posterior draws of the variance-covariance matrix.
# IRFS: posterior draws for the impulse response functions.
# SHOCKS: posterior draes of the shock. Dimension of the array: T-nlags.
summary(model_uhlig)
# Check the number of rejected draws. If you see many rejected draws, the model is poorly specified.
############################
# Bayesian VAR IRFs
############################
irfs <- model_uhlig$IRFS
labels <- c("Inflation","Unemployment","Federal Funds Rate")
# Plot the median of the impulse response draws and plot them, accompanied by confidence bands.
par(mar=c(2,2,2,2))  # Regulate margins
par(mfrow=c(1,1))    # Reset environment
irfplot(irfdraws=irfs, type="median", labels=labels, save=FALSE, bands=c(0.16, 0.84),
grid=TRUE, bw=FALSE)
barplot(model_uhlig$BDraws)
warnings()
model_uhlig$BDraws
BDraws<- model_uhlig$BDraws
barplot(BDraws)
library(tidyverse)
BDraws<- model_uhlig$BDraws %>% as.matrix()
barplot(BDraws)
BDraws<- model_uhlig$BDraws[,2] %>% as.matrix()
View(BDraws)
BDraws<- model_uhlig$BDraws[] %>% as.data.frame()
barplot(BDraws)
View(BDraws)
density(BDraws)
BDraws<- model_uhlig$BDraws
density(BDraws)
# Now I re-estimate the model using Uhlig's (2005) penalty function method.
model_penalty <- uhlig.penalty(Y=swdata, nlags=12, draws=2000, subdraws=1000,
nkeep=1000, KMIN=1, KMAX=6, constrained=constr,
constant=FALSE, steps=60, penalty=100, crit=0.001)
summary(model_penalty )
# Plot IRfs.
irfs_model_penalty <- model_penalty$IRFS
irfplot(irfdraws=irfs_model_penalty, type="median", labels=labels, save=FALSE, bands=c(0.16, 0.84),
grid=TRUE, bw=FALSE)
View(model_penalty)
model_penalty <- uhlig.penalty(Y=swdata, nlags=12, draws=2000, subdraws=1000,
nkeep=1000, KMIN=1, KMAX=6, constrained=constr,
constant=FALSE, steps=60, penalty=100, crit=0.001)
model_uhlig <- uhlig.reject(Y=swdata, nlags=12, draws=200, subdraws=100, nkeep=1000, KMIN=1,
KMAX=6, constrained=constr, constant=FALSE, steps=60)
warnings()
View(model_uhlig)
irfs <- model_uhlig$IRFS
labels <- c("Inflation","Unemployment","Federal Funds Rate")
# Plot the median of the impulse response draws and plot them, accompanied by confidence bands.
par(mar=c(2,2,2,2))  # Regulate margins
par(mfrow=c(3,1))    # Reset environment
irfplot(irfdraws=irfs, type="median", labels=labels, save=FALSE, bands=c(0.16, 0.84),
grid=TRUE, bw=FALSE)
par(mfrow=c(1,1))    # Reset environment
par(mfrow=c(3,1))    # Reset environment
par(mar=c(2,2,2,2))  # Regulate margins
par(mfrow=c(3,1))    # Reset environment
irfplot(irfdraws=irfs, type="median", labels=labels, save=FALSE, bands=c(0.16, 0.84),
grid=TRUE, bw=FALSE)
par(mfrow=c(1,1))    # Reset environment
# This package depends on other three packages
# install.packages("minqa")  # implement the UOBYQA algorithm to minimise Uhlig's penalty function
# install.packages("HI")     # Draws to create the orthogonal impulse vector alpha.
# install.packages("mvnfast")# Draws for the coefficient of the model from the multivariate normal.
# Install VARsignR
# install.packages("VARsignR")
#############
# Library
library(VARsignR) # by Christian Danne
