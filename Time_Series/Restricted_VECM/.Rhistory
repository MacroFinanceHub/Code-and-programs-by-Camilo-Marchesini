return(mat)
}
roberto(20,20,100)
######################
# Library
library(tidyverse)
library(zoo) # zoo contains the function as.yearqtr
library(purrr) # for map_dfr
library(lemon) # Better legend in ggplot2
# Use this package
# install.packages("fredr")
library(fredr)
library(lubridate)
######################
rm(list=ls())
# Risk subindex
gdp <- fredr_series_observations(   # financial stress
series_id = "GDP",
observation_start = as.Date("1995-01-01"),
frequency = "q",
units = "lin"
)
# Make quarterly data as YYYYQ
gdp  <- transform(gdp, date = as.yearqtr(date))
# Credit subindex
potgdp <- fredr_series_observations(   # financial stress
series_id = "NGDPPOT",
observation_start = as.Date("1995-01-01"),
frequency = "q",
units = "lin"
)
# Make quarterly data as YYYYQ
potgdp  <- transform(potgdp, date = as.yearqtr(date))
fredr_set_key("668409be20b0f03136a4f47a921c680c")
###########
rm(list=ls())
# Risk subindex
gdp <- fredr_series_observations(   # financial stress
series_id = "GDP",
observation_start = as.Date("1995-01-01"),
frequency = "q",
units = "lin"
)
# Make quarterly data as YYYYQ
gdp  <- transform(gdp, date = as.yearqtr(date))
# Credit subindex
potgdp <- fredr_series_observations(   # financial stress
series_id = "NGDPPOT",
observation_start = as.Date("1995-01-01"),
frequency = "q",
units = "lin"
)
# Make quarterly data as YYYYQ
potgdp  <- transform(potgdp, date = as.yearqtr(date))
View(gdp)
View(potgdp)
potgdp <- fredr_series_observations(   # financial stress
series_id = "NGDPPOT",
observation_start = as.Date("1995-01-01"),
observation_end = as.Date("2019-01-01"),
frequency = "q",
units = "lin"
)
# Credit subindex
potgdp <- fredr_series_observations(   # financial stress
series_id = "NGDPPOT",
observation_start = as.Date("1995-01-01"),
observation_end = as.Date("2018-12-01"),
frequency = "q",
units = "lin"
)
View(potgdp)
potgdp  <- transform(potgdp, date = as.yearqtr(date))
View(potgdp)
gap <- potgdp-gdp
gap <- potgdp[,3]-gdp[,3]
perc_gap <- (gdp[,3]-potgdp[,3])/potgdp[,3]
gap <- rbind(gdp[,1:2],potgdp)
gap <- rbind(gdp[,1:2],gap)
View(gap)
gap <- gdp[,3]-potgdp[,3]
gap <- gdp[,3]-potgdp[,3] %>% as.data.frame()
gap$series_id <- rep("GDP_GAP", nrow(gap))
View(gap)
gap <- rbind(gdp[,1],gap)
View(gap)
gap <- rbind(gdp[,1],gap)
View(gap)
# I build a function to create nice date formats
make_monthly <- function(start,end,rows,columns,dataset,vardate){
months.list <- c("01","02","03","04","05","06","07","08","09","10","11","12")
years.list <- c(start:end)
#The next line of code generates all the month-year combinations.
df<-expand.grid(year=years.list,month=months.list)
#Then, we paste together the year and month with a day so that we get dates like "2007-03-01".  Pass that to as.Date, and pass the result to as.yearqtr.
df$Date=as.yearmon(as.Date(paste0(df$year,"-",df$month,"-01")))
df <- df[order(df$Date),]
df <- df[rows,columns]
date <- df
dataset$date <- date
# Correct Formatting
dataset$date <- as.yearmon(dataset$date)
dataset$series_id <- as.character(dataset$series_id)
dataset$value <- as.numeric(as.character(dataset$value))
return(dataset)
}
View(gap)
gap <- gdp[,3]-potgdp[,3] %>% as.data.frame()
gap$series_id <- rep("GDP_GAP", nrow(gap))
gap <- rbind(gdp[,1],gap)
colnames(gap) <- c("value","series_id")
gap <- rbind(gdp[,1],gap)
View(gap)
gap <- gdp[,3]-potgdp[,3] %>% as.data.frame()
gap$series_id <- rep("GDP_GAP", nrow(gap))
colnames(gap) <- c("value","series_id")
View(gap)
gap$date <- gdp[,1]
View(gap)
gap <- cbind([,3],[,2],[,1])
library(tidyverse) # many packages bundled into one
camilo <- function(x,y){
z <- rbind(x,y)
return(z)
}
knitr::opts_chunk$set(echo = FALSE,
message = FALSE,
warning = FALSE,
cache = T,
fig.align="center")
library(FE)
library(TTR,quantmod)
library(dlm)
library(tidyverse)
library(stats)
library(forecast)
library(knitr)
library(kableExtra)
library(gridExtra)
library(psych)
# install.packages("EnvStats")
library(EnvStats) # for Q-Q plot
library(YieldCurve)
# install.packages("sarima")
library(sarima)
library(aTSA)
library(rugarch)
# install.packages("fGarch")
library(fGarch)
library(tseries)
library(urca)
library(graphics)
library(MASS)
library(strucchange)
library(sandwich)
library(lmtest)
library(vars)
library(tsDyn)
library(fUnitRoots)
data <- read.table("Dividends.dat", header=T, stringsAsFactors=F)
data <- ts(data[,2:61], start = c(1966, 1), frequency = 4)
dps <- data[,1:15]
eps <- data[,16:30]
prices <- data[,31:45]
# Equally-weighted average
make_index <- function(series){
ind <- apply(series,1,mean) %>%
ts(start = c(1966, 1), frequency = 4)
return(ind)
}
# Price index
pindex  <- make_index(prices)
# Dividends
dpsindex <- make_index(dps)
# EPS
epsindex <- make_index(eps)
# Log dividend (net) growth rate
div_growth <- (diff(log(dpsindex))) %>%
ts(start = c(1966, 2), frequency = 4)
# Log dividend-price ratio
dp_ratio <- log(dpsindex/pindex) %>%
ts(start = c(1966, 1), frequency = 4)
# Earnings-price ratio
ep_ratio <- (epsindex/pindex) %>%
ts(start = c(1966, 1), frequency = 4)
par(mar=c(2,2,2,2))  # Regulate margins
par(mfrow=c(1,1))    # Reset environment
ggtsdisplay(div_growth, main="Log Dividend Growth Rate", points = F,
theme = theme_minimal())
ggtsdisplay(dp_ratio, main="Log Dividend-Price Ratio", points = F,
theme = theme_minimal())
ggtsdisplay(ep_ratio, main="Earnings-Price Ratio", points = F,
theme = theme_minimal())
# Long-Horizon Regressions div_growth (moving (=overlapping) log div_growth)
# Popular forecasting variables: slide 24/45, Lecture 6
# Price-dividend ratio
pd_ratio <- (pindex/dpsindex)
# Price-earnings ratio
pe_ratio <- (pindex/epsindex)
# Spread between long- and short-term interst rates
# Start from quarter 1 of 1982 for simplicity in calculating quarters upon aggregation
data("FedYieldCurve")
FedYieldCurve <- FedYieldCurve[2:nrow(FedYieldCurve),]
monthly <- ts(FedYieldCurve,start=c(1982,1),frequency=12)
quarterly <- aggregate(monthly, nfrequency=4)
# R_10Y  -  R_3M
spread <- quarterly[,1] - quarterly[,8]
# Function to compute Long-Horizon Regressions
lr_pred <- function(series,horizon){
tmp <- matrix(NA,nrow( 100*(diff(log(dpsindex),lag=horizon)) %>% as.matrix() %>%
ts(start = c(1966, 2), frequency = 4)),1)
tmp <- 100*(diff(log(dpsindex),lag=horizon)) %>% as.matrix() %>%
ts(start = c(1966, 2), frequency = 4)
tmpp <- as.matrix(series) %>%
ts(start = c(1966, 2), frequency = 4, end = end(tmp))
tmppp <- cbind(horizon,t(summary(lm(tmp~tmpp))[["coefficients"]][2,]),
summary(lm(tmp~tmpp))[["r.squared"]])
#summary(lm(tmp~tmpp))[["r.squared"]])
return(tmppp)
}
# It would also be possible to compute HAC-robust statistics and coefficient estimates
# I omit the step for brevity.
# lm(tmp~tmpp)
# coeftest(fit,vcov=NeweyWest(fit,verbose=T))
# Create output
out <- function(series){
horizons <- c(8, 20, 40, 48, 60)
# 2-year, 5-year, 10-years, 12-year, 15-year horizons
tmpmat <- matrix(NA, 5, 6)
colnames(tmpmat) <- c("Horizon (Quarters)","Coeff.", "SD", "t-stat", "p-value", "R-sq.")
for (iter in 1:5){
tmpmat[iter,]<- lr_pred(series,horizons[iter])
}
return(tmpmat)
}
# Only for spreads, since the time series is shorter I adjust the function a bit
lr_pred_spread <- function(series,horizon){
tmp <- matrix(NA,nrow( 100*(diff(log(dpsindex),lag=horizon)) %>% as.matrix() %>%
ts(start = c(1982, 1), frequency = 4)),1)
tmp <- 100*(diff(log(dpsindex),lag=horizon)) %>% as.matrix() %>%
ts(start = c(1982, 1), frequency = 4)
tmpp <- as.matrix(series) %>%
ts(start = c(1982, 2), frequency = 4, end = end(tmp))
tmppp <- cbind(horizon,t(summary(lm(tmp~tmpp))[["coefficients"]][2,]),
summary(lm(tmp~tmpp))[["r.squared"]])
#summary(lm(tmp~tmpp))[["r.squared"]])
return(tmppp)
}
# Create output
out_spread <- function(series){
horizons <- c(8, 20, 40)
tmpmat <- matrix(NA, 3, 6)
colnames(tmpmat) <- c("Horizon","Coeff.", "SD", "t-stat", "p-value", "R-sq.")
for (iter in 1:3){
tmpmat[iter,]<- lr_pred(series,horizons[iter])
}
return(tmpmat)
}
lr_output <- rbind(
out(pd_ratio),
out(pe_ratio),
out(dp_ratio),
out_spread(spread)
)
kable(lr_output, digits = 4, format = "html", caption = "Long-Horizon Regressions (Dep. Var: Log Dividend Growth Rate)",
align = "lccccc") %>%
kable_styling(c("striped","condensed")) %>%
group_rows("Panel A: Price-Dividend Ratio", 1, 5, label_row_css = "background-color: #521; color: #fff;") %>%
group_rows("Panel B: Price-Earnings Ratio",6, 10, label_row_css = "background-color: #521; color: #fff;") %>%
group_rows("Panel C: Log Dividend-Price Ratio",11, 15, label_row_css = "background-color: #521; color: #fff;") %>%
group_rows("Panel D: Yield Spread",16, 18, label_row_css = "background-color: #521; color: #fff;") %>%
#row_spec(c(2:5,12:15), bold = T, color = "white", background = "#367") %>%
scroll_box(width = "100%", height = "400px")
# VAR
returns <- data[,46:ncol(data)]; retindex <- (make_index(returns))
# Log returns, log dividend growth rate, E/P ratio and log D/P ratio
vardata <- (cbind(retindex, div_growth,ep_ratio,dp_ratio))
vardata <- na.omit(vardata)
# Fit VAR(1) + constant
varFE <- VAR(vardata, p = 1, type = "const")
# The Forecast (2 quarters) error variance decomposition is more interesting than the estimated coefficients
vdecompo <- matrix(NA,8,5)
vdecompo[,1] <- hor <- rep(c(1,2),4)
colnames(vdecompo) <-c("Quarters Ahead","Log Returns","Log Div. Growth Rate","E/P Ratio", " Log D/P Ratio")
vdecompo[1:2,2:5] <- fevd(varFE, n.ahead = 2)$retindex*100
vdecompo[3:4,2:5] <- fevd(varFE, n.ahead = 2)$div_growth*100
vdecompo[5:6,2:5] <- fevd(varFE, n.ahead = 2)$ep_ratio*100
vdecompo[7:8,2:5] <- fevd(varFE, n.ahead = 2)$dp_ratio*100
kable(vdecompo, digits = 4, format = "html", caption = "Forecast Error Variance decomposition",
align = "lcccc", col.names = colnames(vdecompo))  %>%
kable_styling(c("striped","condensed")) %>%
group_rows("Panel A: Log Returns", 1, 2, label_row_css = "background-color: #333; color: #fff;") %>%
group_rows("Panel B: Log Dividend Growth Rate",3, 4, label_row_css = "background-color: #333; color: #fff;") %>%
group_rows("Panel C: Earnings-Price Ratio",5, 6, label_row_css = "background-color: #333; color: #fff;") %>%
group_rows("Panel D: Dividend-Price Ratio",7, 8, label_row_css = "background-color: #333; color: #fff;") %>%
scroll_box(width = "100%", height = "400px")
summary_VAR <- summary(varFE)[["corres"]] %>% as.matrix()
colnames(summary_VAR) <- c("Log Returns","Log Div. Growth Rate","E/P Ratio", " Log D/P Ratio")
rownames(summary_VAR) <- colnames(summary_VAR)
kable(summary_VAR, digits = 4, format = "html", caption = "Correlation Matrix VAR(1)",
align = "lcccc") %>%
kable_styling(c("striped","condensed")) %>%
scroll_box(width = "100%", height = "300px")
Research_Interests <-c("Macroeconometrics","Monetary Economics",
"International Finance","Computational Macroeconomics")
Research_Interests
Research_Interests <-as.matrix(c("Macroeconometrics","Monetary Economics",
"International Finance","Computational Macroeconomics"))
View(Research_Interests)
solve_macroproblems <- matrix(NA,5,1)
for (iter in 1:5){
solve_macroproblems[iter,] <- 6
}
solve_macroproblems
solve_macroproblems <- matrix(NA,5,1)
for (iter in 1:5){
solve_macroproblems[iter] <- 6
}
solve_macroproblems
setwd("~/codes_github/Restricted_VECM")
################################################
# This program was written by Camilo Marchesini.
################################################
# This file offers an example on how to test for cointegration using a restricted VECM approach.
# Are the Swedish STIBOR and the 10-year Swedish government bonds cointegrated series?
# With the restricted VECM approach I use Johansen's procedure to test whether the spread between
# the two is stationary by assuming cointegration by imposing rescritions on the coefficient of the VECM representation.
# MIT License
#
# Copyright (c) 2019 Camilo Marchesini
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
#   The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
# My opening routine
########
rm(list=ls()) # clean global environment
setwd("yourworkingdirectory")
########
# Library
library(forecast)
library(ggplot2)
library(tseries)
library(urca) # contains -ca.jo function
library(graphics)
library(MASS)
library(strucchange)
library(sandwich)
library(lmtest)
library(vars)
library(tsDyn)
library(fUnitRoots)
############
# Data: September 1987 to September 2018
##########
# Set-up
cointegrationdata <- read.delim2("cointegration.txt")
head(cointegrationdata) # Quick look at the data
swe <- ts(cointegrationdata[,c(2,3)],
start = c(1987,10), end= c(2018,8), frequency = 12)
stibor <- swe[,1]
gvb    <- swe[,2]
# Plot the series
leg.text <- c("Stibor", "Government bonds")
ts.plot(swe,
main="Stibor and Government bonds, monthly data",
ylab="Rate",
col= c("blue","red"),
lty = 1 , lwd = 3)
legend("topright", leg.text, col= c("blue","red"),
lty = 1, lwd =3, box.lty=0, box.lwd = 0, cex = 1) # no box around
axis(side = 1, at=c(1987,2018),  padj=1)          # Adjusting the x-axis
###################################################
# Select optimal lag length in VAR
#################################################
lag.max <- c(12,8,6) # Testing in decreasing order
tmp <- matrix(NA, 3, 4)
for (i in 1:3 ){
tmp[i,] <- VARselect(swe, lag.max = lag.max[i], type = "const")$selection
}
colnames(tmp) <- c("AIC","HQ","SC","FPE")
row.names(tmp) <- c("12","8","6")
kable(tmp, digits = 1, format = "latex",
caption = "Lag Order Choices for Different Information Criteria",
align = "l",
booktabs=T)
# I decide to choose lag length in accordance to the AIC. I stick to this choice throughout the rest of
# the code
###################################
# Testing univariate unit roots
#################################
# To check for cointegration, first I check whether
# both series contain a stochastic trend.
# AdfTests performs the tests using no constant, constant or constant and trend
# which correspond to Case 1 (Pure RW), Case 2 (Drift) and Case 4 (Drift+trend) in Hamilton 1994 (pp.475-543)
# Test Stibor
a <- cbind(adfTest(stibor, lags = 4, type = "nc")@test$statistic,
adfTest(stibor, lags = 4, type = "nc")@test$p.value)
b <- cbind(adfTest(stibor, lags = 4, type = "c")@test$statistic,
adfTest(stibor, lags = 4, type = "c")@test$p.value)
c <- cbind(adfTest(stibor, lags = 4, type = "ct")@test$statistic,
adfTest(stibor, lags = 4, type = "ct")@test$p.value)
unitr.stib <- rbind(a,b,c)
row.names(unitr.stib) <- c("Pure RW","Drift","Drift+trend")
kable(unitr.stib, digits = 2, format = "latex",
caption = "Univariate Unit Root Tests, Stibor",
align = "l",
booktabs=T)
# Exactly the same for government bonds
d <- cbind(adfTest(gvb, lags = 4, type = "nc")@test$statistic,
adfTest(gvb, lags = 4, type = "nc")@test$p.value)
e <- cbind(adfTest(gvb, lags = 4, type = "c")@test$statistic,
adfTest(gvb, lags = 4, type = "c")@test$p.value)
f <- cbind(adfTest(gvb, lags = 4, type = "ct")@test$statistic,
adfTest(gvb, lags = 4, type = "ct")@test$p.value)
unitr.gvb <- rbind(d,e,f)
row.names(unitr.gvb) <- row.names(unitr.stib)
kable(unitr.gvb, digits = 2, format = "latex",
caption = "Univariate Unit Root Tests, Government Bonds",
align = "l",
booktabs=T)
###################################
# Estimating the cointegrating rank
###################################
##################
# Johansen
#################
# I specify the (unrestricted) VECM in its long-run representation
# and use both the trace and the maximum eigenvalue test.
# I also check whether anything changes when I change the nature of the deterministic trend
# K=4 is the lag length
jotest.tr1 <-ca.jo(swe, type="trace", K=4, ecdet="const", spec="longrun")
test1 <- cbind(jotest.tr1@teststat,jotest.tr1@cval)
jotest.tr2 <-ca.jo(swe, type="trace", K=4, ecdet="trend", spec="longrun")
test2 <- cbind(jotest.tr2@teststat,jotest.tr2@cval)
jotest.eigen1 <-ca.jo(swe, type="eigen", K=4, ecdet="const", spec="longrun")
test3 <- cbind(jotest.eigen1@teststat,jotest.eigen1@cval)
jotest.eigen2 <-ca.jo(swe, type="eigen", K=4, ecdet="trend", spec="longrun")
test4 <- cbind(jotest.eigen2@teststat,jotest.eigen2@cval)
jo.tests <- rbind(test1,test2,test3,test4)
row.names(jo.tests) <- c("r less or equal to 1","r equal to 0",
"r less or equal to 1","r equal to 0",
"r less or equal to 1","r equal to 0",
"r less or equal to 1","r equal to 0")
kable(jo.tests, digits = 2, format = "latex",
caption = "Estimation of the Cointegrating Rank of the VAR using Johansen procedure",
align = "l",
col.names = c("10pct","5pct","1pct"),
booktabs=T) %>%
add_header_above(c("Null hypothesis", "t-statistic" = 1, "Critical values" = 3))%>%
# critical values from Osterwald-Lenum, 1992
group_rows("Panel A: Johansen trace test, with drift", 1, 2) %>%
group_rows("Panel B: Johansen trace test, with trend",3, 4) %>%
group_rows("Panel C: Johansen max.eigenvalue test, with drift",5, 6) %>%
group_rows("Panel D: Johansen max.eigenvalue test, with trend", 7, 8)
############################
# Restricted VECM
############################
# I conduct inference on the coefficients of a restricted VECM; H0: cointegration
# Fit the VECM (K=estimated number of lags in VAR in levels)
jotest.trVECM1 <- ca.jo(swe, type="trace", K=4, ecdet="none", spec="longrun")
# Restricting matrix (linear restrictions on beta)
B1 <- matrix(c(1,-1), nrow = 2)
# Test using likelihood ratio
testing1 <- blrtest(z = jotest.trVECM1 , B1, r=1)
jotest.trVECM2 <-ca.jo(swe, type="eigen", K=4, ecdet="none", spec="longrun")
testing2 <- blrtest(z = jotest.trVECM2 , B1, r=1)
# Need to take transpose of @pval to show degrees of freedom
bigtest <- rbind(cbind(testing1@teststat,t(testing1@pval)),
cbind(testing2@teststat,t(testing2@pval)))
kable(bigtest, digits = 2, format = "latex",
caption = "Likelihood-ratio test on restricted VECM",
align = "lcc",
col.names = c("t-statistic","p-value","Degrees of freedom"),
booktabs=T) %>%
group_rows("Trace test", 1, 1) %>%
group_rows("Max. Eigenvalue test",2, 2)
# Transform VEC to VAR with r = 1
cointegrated_var <- vec2var(jotest.tr1, r = 1)
# Obtain IRF
ir <- irf(cointegrated_var, n.ahead = 20, impulse = "stibor", response = "gvb",
ortho = FALSE, runs = 500)
# Plot
plot(ir)
library(kableExtra)
# Transform VEC to VAR with r = 1
cointegrated_var <- vec2var(jotest.tr1, r = 1)
# Obtain IRF
ir <- irf(cointegrated_var, n.ahead = 20, impulse = "STIBOR.1W", response = "SE.GVB.10Y",
ortho = FALSE, runs = 500)
# Plot
plot(ir)
cointegrated_var <- vec2var(jotest.tr1, r = 1)
# Obtain IRF
ir <- irf(cointegrated_var, n.ahead = 20, impulse = "SE.GVB.10Y", response = "STIBOR.1W",
ortho = FALSE, runs = 500)
# Plot
plot(ir)
